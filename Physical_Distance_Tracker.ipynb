{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Physical Distance Tracker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORLBgisGNutGza3MNuM3xh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IqbalLx/Physical-Distnace-Tracker/blob/master/Physical_Distance_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS8KPG4vnXAr",
        "colab_type": "code",
        "outputId": "80f7e9be-134a-41b2-d95a-e8895873d720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwin860voBB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sefM-OhTogqR",
        "colab_type": "code",
        "outputId": "296309b2-1cb4-4eef-d249-7b03e77170ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive/physicaldistance'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/physicaldistance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfV0vcvRTp2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_dist(point_a, point_b):\n",
        "  return np.sqrt((point_a[1] - point_b[1])**2 + (point_a[0] - point_b[0])**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6twv2gG_OzbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transforming into bird's eye view\n",
        "def transform(img, points):\n",
        "    #Ordering points\n",
        "    #rect = order_points(points)\n",
        "    tl, tr, br, bl = points\n",
        "\n",
        "    #Construct bird's eye image matrix\n",
        "    #I want it vertical, so it'll find minimum width\n",
        "    #between distance for tl to tr and bl to br\n",
        "    upperW = euclidean_dist(tl, tr)\n",
        "    lowerW = euclidean_dist(bl, br)\n",
        "    minW = min(upperW, lowerW)\n",
        "\n",
        "    #And max height between distance for tl to bl\n",
        "    #and tr to br\n",
        "    upperH = euclidean_dist(tl, bl)\n",
        "    lowerH = euclidean_dist(tr, br)\n",
        "    maxH = max(upperH, lowerH)\n",
        "\n",
        "    #Put all the information\n",
        "    tmp = np.array([\n",
        "        [0, 0],\n",
        "        [minW-1, 0],\n",
        "        [minW-1, maxH-1],\n",
        "        [0, maxH-1]\n",
        "    ], dtype='float32')\n",
        "\n",
        "    #Warping and transform\n",
        "    M = cv2.getPerspectiveTransform(points, tmp)\n",
        "    warped = cv2.warpPerspective(img, M, (int(minW), int(maxH)))\n",
        "\n",
        "    return warped, M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klk3AZLNnQ2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------------------------\n",
        "#\n",
        "#Code originated from https://gist.github.com/madhawav/1546a4b99c8313f06c0b2d7d7b4a09e2\n",
        "#Tensorflow Human Detection - with minor tweaks\n",
        "#\n",
        "#-----------------------------\n",
        "\n",
        "class DetectorAPI:\n",
        "    def __init__(self, path_to_ckpt):\n",
        "        self.path_to_ckpt = path_to_ckpt\n",
        "\n",
        "        self.detection_graph = tf.Graph()\n",
        "        with self.detection_graph.as_default():\n",
        "            od_graph_def = tf.GraphDef()\n",
        "            with tf.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "        self.default_graph = self.detection_graph.as_default()\n",
        "        self.sess = tf.Session(graph=self.detection_graph)\n",
        "\n",
        "        # Definite input and output Tensors for detection_graph\n",
        "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "        # Each box represents a part of the image where a particular object was detected.\n",
        "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "        # Each score represent how level of confidence for each of the objects.\n",
        "        # Score is shown on the result image, together with the class label.\n",
        "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "    def processFrame(self, image):\n",
        "        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
        "        image_np_expanded = np.expand_dims(image, axis=0)\n",
        "        # Actual detection.\n",
        "        (boxes, scores, classes, num) = self.sess.run(\n",
        "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
        "            feed_dict={self.image_tensor: image_np_expanded})\n",
        "\n",
        "        im_height, im_width,_ = image.shape\n",
        "        boxes_list = [None for i in range(boxes.shape[1])]\n",
        "        for i in range(boxes.shape[1]):\n",
        "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
        "                        int(boxes[0,i,1]*im_width),\n",
        "                        int(boxes[0,i,2] * im_height),\n",
        "                        int(boxes[0,i,3]*im_width))\n",
        "\n",
        "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
        "\n",
        "    def close(self):\n",
        "        self.sess.close()\n",
        "        self.default_graph.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQrngGM1APVT",
        "colab_type": "text"
      },
      "source": [
        "You can run one of this cell, each output is defined by the title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118r7GhB_YDH",
        "colab_type": "text"
      },
      "source": [
        "## Normal view video with Human bounding box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEqRJEnuMCCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = 'faster_rcnn_nas/frozen_inference_graph.pb'\n",
        "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "threshold = 0.7\n",
        "cap = cv2.VideoCapture('sample.mp4')\n",
        "baseW = 640\n",
        "res=(baseW, 360) #resolution horizontal\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n",
        "out = cv2.VideoWriter('video-out-nas.mp4', fourcc, 20.0, res)\n",
        "\n",
        "while True:\n",
        "  r, img = cap.read()\n",
        "  if r:\n",
        "      height, width = img.shape[:2]\n",
        "      ratio = baseW / width\n",
        "      newH = int(ratio * height)\n",
        "      img = cv2.resize(img, (baseW, newH), interpolation= cv2.INTER_LINEAR)\n",
        "\n",
        "      boxes, scores, classes, num = odapi.processFrame(img)\n",
        "\n",
        "      human_boxes = [boxes[i] for i in range(len(boxes)) if classes[i]==1 and scores[i]>threshold]\n",
        "      for box in human_boxes:\n",
        "          w = box[3] - box[1]\n",
        "          h = box[2] - box[0]\n",
        "          if h > w:\n",
        "            cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0, 255, 0), 2)\n",
        "\n",
        "      out.write(img)\n",
        "  else: \n",
        "      break\n",
        "\n",
        "out.release()\n",
        "cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTusHchA_iiH",
        "colab_type": "text"
      },
      "source": [
        "##Warped view (birds eye view)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRetYfXvMd3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = 'faster_rcnn_nas/frozen_inference_graph.pb'\n",
        "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "threshold = 0.7\n",
        "cap = cv2.VideoCapture('sample.mp4')\n",
        "res=(265, 405) #resulotion vertical\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n",
        "out = cv2.VideoWriter('video-vert.mp4', fourcc, 20.0, res)\n",
        "\n",
        "while True:\n",
        "  r, img = cap.read()\n",
        "  if r:\n",
        "      height, width = img.shape[:2]\n",
        "      baseW = 640\n",
        "      ratio = baseW / width\n",
        "      newH = int(ratio * height)\n",
        "      img = cv2.resize(img, (baseW, newH), interpolation= cv2.INTER_LINEAR)\n",
        "      copy_img = img.copy()\n",
        "\n",
        "      boxes, scores, classes, num = odapi.processFrame(img)\n",
        "\n",
        "      human_boxes = [boxes[i] for i in range(len(boxes)) if classes[i]==1 and scores[i]>threshold]\n",
        "      warp_img, mat = transform(copy_img, np.array([[353, 11], [618, 13], [530, 350], [4, 217]], dtype='float32'))\n",
        "      for box in human_boxes:\n",
        "          x= box[1]\n",
        "          w = box[3] - box[1]\n",
        "          h = box[2] - box[0]\n",
        "          if h > w:\n",
        "            points = [x + (w//2), box[2]]\n",
        "            src = np.array(points, dtype='float32').reshape(1, 1, 2)\n",
        "            dst = np.zeros((1, 2), dtype='float32')\n",
        "\n",
        "            warp_pts = cv2.perspectiveTransform(src, mat, dst).reshape(2,)\n",
        "            cv2.circle(warp_img, (warp_pts[0], warp_pts[1]), 5, (0, 0, 255), 3)\n",
        "\n",
        "      out.write(warp_img)\n",
        "  else: \n",
        "      break\n",
        "\n",
        "out.release()\n",
        "cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V5VnL69_syK",
        "colab_type": "text"
      },
      "source": [
        "##Black vertical video with point mapped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2g9EfEHpvRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = 'faster_rcnn_nas/frozen_inference_graph.pb'\n",
        "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "threshold = 0.7\n",
        "cap = cv2.VideoCapture('sample.mp4')\n",
        "res=(265, 405) #resulotion vertical\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n",
        "out = cv2.VideoWriter('video-black.mp4', fourcc, 20.0, res)\n",
        "\n",
        "while True:\n",
        "  r, img = cap.read()\n",
        "  if r:\n",
        "      height, width = img.shape[:2]\n",
        "      baseW = 640\n",
        "      ratio = baseW / width\n",
        "      newH = int(ratio * height)\n",
        "      img = cv2.resize(img, (baseW, newH), interpolation= cv2.INTER_LINEAR)\n",
        "      copy_img = img.copy()\n",
        "\n",
        "      boxes, scores, classes, num = odapi.processFrame(img)\n",
        "\n",
        "      human_boxes = [boxes[i] for i in range(len(boxes)) if classes[i]==1 and scores[i]>threshold]\n",
        "      warp_img, mat = transform(copy_img, np.array([[353, 11], [618, 13], [530, 350], [4, 217]], dtype='float32'))\n",
        "      black_img = np.zeros((warp_img.shape[0], warp_img.shape[1], 3), np.uint8)\n",
        "      for box in human_boxes:\n",
        "          x= box[1]\n",
        "          w = box[3] - box[1]\n",
        "          h = box[2] - box[0]\n",
        "          if h > w:\n",
        "            points = [x + (w//2), box[2]]\n",
        "            src = np.array(points, dtype='float32').reshape(1, 1, 2)\n",
        "            dst = np.zeros((1, 2), dtype='float32')\n",
        "\n",
        "            warp_pts = cv2.perspectiveTransform(src, mat, dst).reshape(2,)\n",
        "            cv2.circle(black_img, (warp_pts[0], warp_pts[1]), 5, (0, 0, 255), 3)\n",
        "\n",
        "      out.write(black_img)\n",
        "  else: \n",
        "      break\n",
        "\n",
        "out.release()\n",
        "cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-8OkyB_1pm",
        "colab_type": "text"
      },
      "source": [
        "##Measured distance video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmoslL0TV7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = 'faster_rcnn_nas/frozen_inference_graph.pb'\n",
        "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "threshold = 0.4\n",
        "cap = cv2.VideoCapture('sample.mp4')\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n",
        "#Black vid\n",
        "res=(265, 405) #resulotion vertical\n",
        "out = cv2.VideoWriter('video-black-measureed.mp4', fourcc, 20.0, res)\n",
        "\n",
        "#Measured normal vid\n",
        "res_norm=(640, 360) #resolution horizontal\n",
        "out_norm = cv2.VideoWriter('video-out-measured.mp4', fourcc, 20.0, res_norm)\n",
        "\n",
        "ref_pts = [[353, 11], [618, 13], [530, 350], [4, 217]]\n",
        "counter = 0\n",
        "while True:\n",
        "  r, img = cap.read()\n",
        "  if r:\n",
        "      height, width = img.shape[:2]\n",
        "      baseW = 640\n",
        "      ratio = baseW / width\n",
        "      newH = int(ratio * height)\n",
        "      img = cv2.resize(img, (baseW, newH), interpolation= cv2.INTER_LINEAR)\n",
        "      copy_img = img.copy()\n",
        "\n",
        "      boxes, scores, classes, num = odapi.processFrame(img)\n",
        "\n",
        "      human_boxes = [boxes[i] for i in range(len(boxes)) if classes[i]==1 and scores[i]>threshold]\n",
        "      warp_img, mat = transform(copy_img, np.array(ref_pts, dtype='float32'))\n",
        "      black_img = np.zeros((warp_img.shape[0], warp_img.shape[1], 3), np.uint8)\n",
        "\n",
        "      coord_points = []\n",
        "      centroid = []\n",
        "      for box in human_boxes:\n",
        "          #For easiness\n",
        "          x= box[1]\n",
        "          w= box[3] - x\n",
        "          y= box[0]\n",
        "          h= box[2] - y \n",
        "          if h > w:\n",
        "            #Human boxes' centroid in img (normal image)\n",
        "            cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0, 255, 0), 3)\n",
        "            ctr= [x + (w//2), y + (h//2)]\n",
        "            centroid.append(ctr)\n",
        "\n",
        "            #Perspective transformation foot circle in black_img\n",
        "            points = [x + (w//2), box[2]]\n",
        "            src = np.array(points, dtype='float32').reshape(1, 1, 2)\n",
        "            dst = np.zeros((1, 2), dtype='float32')\n",
        "            warp_pts = cv2.perspectiveTransform(src, mat, dst).reshape(2,)\n",
        "\n",
        "            current_a, current_b = warp_pts[:2]\n",
        "            coord_points.append([current_a, current_b])\n",
        "            cv2.circle(black_img, (current_a, current_b), 5, (0, 255, 0), 3)\n",
        "\n",
        "      coord_points = np.array(coord_points, dtype='float32')\n",
        "      distances = np.linalg.norm(coord_points - coord_points[:,None], axis=-1)\n",
        "      distances = np.where(distances == 0, np.nan, distances)\n",
        "\n",
        "      violation = np.where((distances < 30) & (distances > 15))\n",
        "      if len(violation[0]) !=0:\n",
        "        for violate in violation:\n",
        "          x, y = violate[0], violate[1]\n",
        "          cv2.line(img, tuple(centroid[x]), tuple(centroid[y]), (0, 0, 255), 2)\n",
        "          cv2.line(black_img, tuple(coord_points[x]), tuple(coord_points[y]), (0, 0, 255), 2)\n",
        "          counter += 1\n",
        "      \n",
        "      cv2.putText(img, f\"Close contact: {counter}\", (23, 23), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)\n",
        "      out.write(black_img)\n",
        "      out_norm.write(img)\n",
        "  else: \n",
        "      break\n",
        "\n",
        "out.release()\n",
        "out_norm.release()\n",
        "cap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}